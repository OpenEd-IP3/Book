{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![book header](pictures/header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt           # For plotting purposes\n",
    "import numpy as np                        # For convolution function\n",
    "from scipy.io import wavfile\n",
    "import time\n",
    "\n",
    "# from serial import Serial # Uncomment this line if you are using the real car\n",
    "# from pyaudio import PyAudio, paInt16 # Uncomment this line if you are using the real car\n",
    "from KITT_Simulator.py_audio_simulator import PyAudio, paInt16 # Uncomment this line if you are using the simulator\n",
    "from KITT_Simulator.serial_simulator import Serial # Uncomment this line if you are using the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Locating KITT Using Audio Communication\n",
    "\n",
    "__Learning objectives__ The following is learned and practiced in this module:\n",
    "- The importance of good auto-correlation properties of your bit-code\n",
    "- Channel estimation and TDOA from recorded signals\n",
    "- item Location estimation algorithms based on TDOA\n",
    "\n",
    "    \n",
    "KITT must be located in its field and then directions must be determined to navigate to the final destination. In the previous modules your colleagues are developing scripts to communicate with KITT. They will add functionality to read the audio signals from the microphones located around the field, and you should use these to locate the car. It is recommended for all group members to read Modules 1 and 2 to have a better understanding of how everything should work together.\n",
    "\n",
    "For the localization, we will use (real-time) recordings of the beacon signal at the various microphones, deconvolve these using a reference signal recording, and determine the relative time delays from the resulting channel estimates. (It is assumed that you have working channel estimation algorithms from the\n",
    "EE2T11 Telecommunications A (??@B update) course lab.) Depending on the distance to each microphone, the signal transmitted by KITT’s beacon arrives a little bit earlier or later, and you can convert that into physical distances. For each pair of microphones, we can compute this time difference of arrival (TDOA), or the physical difference in propagation distance. If you have measurements from enough microphones, then you can calculate the location of KITT in the field.\n",
    "\n",
    "At the end of this Module, you will have developed a script to locate KITT within the field with reasonable accuracy, and you will do so using the data recorded by the microphones located along the field. You will also have tested and verified the accuracy and robustness of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-recorded data\n",
    "\n",
    "To get you started, 7 recodings with known locations and a reference recording taken close to the microphone will be provided in this task. This can be used to develop and test your algorithms. These recordings have locations randomly distributed across the field. An example recording with the location are, \n",
    "\n",
    "| |   x   |   y   |\n",
    "|--|-------|-------|\n",
    "|0|  64   |   40  |\n",
    "|1|  82   |  399  |\n",
    "|2| 109  |   76  |\n",
    "|3| 143  |  296  |\n",
    "|4|  150  |  185  |\n",
    "|5|  178  |  439  |\n",
    "|6|  232  |  275  |\n",
    "\n",
    "*Table 1: Locations of the given recordings (cm)*\n",
    "\n",
    "The x and y axe are defined as follows, where the numbers refer to the microphone index:\n",
    "\n",
    "\n",
    " <img src=\"pictures/axisdef.png\" alt=\"mic-figure\" width=\"250px\">\n",
    "\n",
    "*Microphone Axis definition*\n",
    "<!--\n",
    "```{figure} axisdef.png\n",
    "---\n",
    "height/width: 150px\n",
    "name: mic-figure\n",
    "---\n",
    "Microphone Axis definition\n",
    "```\n",
    "-->\n",
    "\n",
    "You can assume these positions for the microphones. Please note the different height of microphone 5.\n",
    "\n",
    "|Microphone|   x   |  y  |  z  |\n",
    "|-------|-------|-------|-------|\n",
    "|  1    |  0    | 0     |   50  |\n",
    "|  2    |  0    | 480   |   50  |\n",
    "|  3    |  480  | 480   |   50  |\n",
    "|  4    |  480  | 0     |   50  |\n",
    "|  5    |  0    | 240   |   80  |\n",
    "\n",
    "*Table 2: Location of the microphones (cm)*\n",
    "\n",
    "The code below helps you to load and plot the 7 audio signals provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_x = [64, 82, 109, 143, 150, 178, 232]\n",
    "record_y = [40, 399, 76, 296, 185, 439, 275]\n",
    "\n",
    "filename =[]\n",
    "\n",
    "for i in range(len(record_x)):\n",
    "    real_x = record_x[i] \n",
    "    real_y = record_y[i]\n",
    "    filename.append(\"Files/Student Recordings/record_x\" + str(real_x) + \"_y\" + str( real_y) + \".wav\")\n",
    "\n",
    "\n",
    "Fs, recording = wavfile.read(filename[0])\n",
    "plt.plot(recording[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Knowledge\n",
    "\n",
    "In the previous course lab (??@B update), you developed algorithms for channel estimation for 2 microphone signals, both receiving a beacon (training) signal. In the present module, we extend this to 5 microphones and use this to locate the car.\n",
    "\n",
    "The channel estimation problem is the following: Suppose we transmit a known signal $x[n]$ over a communication channel, and measure the result $y[n]$.  The channel acts as a\n",
    "filter, which we will assume to be linear and time-invariant.\n",
    "Therefore, the measured signal is a convolution of the transmitted\n",
    "signal by the channel impulse response $h[n]$, such that\n",
    "$y[n] = h[n] \\ast x[n]$.  Knowing the transmitted signal $x[n]$, can\n",
    "we recover the impulse response of the communication channel from\n",
    "$y[n]$?  This is essentially an inversion problem.\n",
    "\n",
    "\n",
    "To estimate $h[n]$, three alternative algorithms were described:\n",
    "- __ch1__ Deconvolution in the time domain: Involves matrix inversion. It is computationally complex and requires lots of memory (easily more than what the available PCs can handle).\n",
    "\n",
    "- __ch2__ The matched filter: Avoids the matrix inversion. It is equal to computing the cross-correlation of\n",
    "$y[n]$ with $x[n]$. As cross-correlation is equivalent to a convolution with a reverse signal $x[−n]$, it can be computed efficiently using the FFT. The resulting channel estimate is equal to the true\n",
    "channel convolved with the autocorrelation of the pulse, $x[n] ∗ x[−n]$. Therefore, its performance\n",
    "heavily depends on having the correct (i.e., accurate) “reference” or “training” signal to correlate\n",
    "your measurements with, and the signal should have good autocorrelation properties: $x[n] ∗ x[−n]$\n",
    "should be close to a delta spike. Large sidelobes will lead to confusion.\n",
    "- __ch3__ Deconvolution in the frequency domain: Involves the FFT. It computes the same channel as via\n",
    "deconvolution in the time domain but is much more efficient: convolution in time becomes point-\n",
    "wise multiplication in frequency, hence for deconvolution in the frequency domain, we only need\n",
    "a pointwise division, followed by an IFFT to obtain the time-domain channel impulse response.\n",
    "\n",
    "```{admonition} Note\n",
    "\n",
    "FFT and IFFT lead to periodicities (the result is cyclic, and samples $x[n]$ at negative time reappear\n",
    "at the “large n” part of the signal). Since this method is similar to ch1 (but much more efficient),\n",
    "you can view this method as a matched filter, followed by a correction step that “inverts” the effect\n",
    "of the autocorrelation of the transmit pulse. However, since we should not divide by zero, you will\n",
    "also need to implement a “threshold” to set non-invertible or very small frequency coefficients of\n",
    "$x[n]$ to zero, which otherwise will lead to noise amplification. The performance depends on this\n",
    "threshold, which has to be chosen heuristically.\n",
    "\n",
    "```\n",
    "It is assumed that you have working algorithms for estimating channel responses. You developed these during the previous labs (??@B). We recommend using __ch3:__ deconvolution in thefrequency domain.\n",
    "\n",
    "In each case, a reference signal $x[n]$ is required. We recommend using a recording close to the beacon because then $x[n]$ includes the loudspeaker and microphone responses as well. For the pre-made recordings, a high-quality reference is available.\n",
    "\n",
    "\n",
    "The deconvolution algorithm gives a channel estimate for the beacon path to each microphone. After this, we detect the first incoming path, which, assuming Line of Sight (LOS), corresponds to the propagation delay of the car beacon to each microphone. Unfortunately, we do not know the transmit time, so we only obtain relative propagation delays. If we take the difference of microphone delays, this unknown transmit time is eliminated, so we can obtain time difference of arrival (TDOA) samples for each microphone pair.\n",
    "In the next sections, we will use these to locate the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution ##\n",
    "\n",
    "The first step is to get your deconvolution algorithm operational and tested. Do these tasks with the given\n",
    "recordings from Brightspace.\n",
    "\n",
    "**Task 1**  Using the reference signal provided and the algorithms you developed at the EE2T11 Telecom-\n",
    "munications A practical, deconvolve the recordings to get the channel impulse response for each\n",
    "microphone. You will need to segment the received data into individual pulses; for the moment,\n",
    "you can do that manually. For a few measured locations, plot examples of the segmented data and the deconvolved channels\n",
    "(e.g., 10 plots per recording: the inputs and outputs of your deconvolution algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first copy/paset your channel estimation from the earlier assignment here\n",
    "def channel(x, y): # this is ch2\n",
    "        \"\"\"\n",
    "        Channel estimation using matched filtering.\n",
    "        \"\"\"\n",
    "        xr = x[::-1]\n",
    "        h = np.convolve(y, xr, mode='full')  # filter xr with y\n",
    "        alpha = np.dot(x.T, x)\n",
    "        hhat = h / alpha\n",
    "        return abs(hhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load, normalize and segment one period of the reference signal\n",
    "Fs, refSignal = wavfile.read(\"Files/Student Recordings/reference.wav\") # reference signal\n",
    "refSignal = refSignal[0:4000,0] # one pulse\n",
    "refSignal = refSignal / max(refSignal) # normalize\n",
    "plt.plot(refSignal)\n",
    "\n",
    "# load, normalize and segment one period of the reference signal\n",
    "Fs, reccordings = wavfile.read(\"Files/Student Recordings/record_x109_y76.wav\")\n",
    "slice_rec1 = reccordings[:20000,0]\n",
    "slice_rec1 = slice_rec1 / max(slice_rec1) # normalize\n",
    "plt.plot(slice_rec1) #one pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find the impulse response using the channel estimation \n",
    "x = channel(refSignal, slice_rec1)\n",
    "x = x / max(x)\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**  From the peaks in the channel estimates, determine the time of arrivals (TOAs) and store these\n",
    "in a table. Check the accuracies of these estimates. Hint: Obviously, you want to compare these\n",
    "estimates to your true TOAs. The problem is that the transmission time is unknown, so a direct\n",
    "comparison is impossible. You can either compare TDOAs (the pairwise differences of TOAs) or\n",
    "introduce a single unknown parameter (the time of transmission) and develop an error measure\n",
    "that is insensitive to it. This is harder, but you might find literature on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: TDOA\n",
    "### load, normalize, segment and estimate the channel for two different channels as before\n",
    "\n",
    "## Load two channels\n",
    "rec1=recording[:, 0]\n",
    "rec2=recording[:, 2]\n",
    "\n",
    "## normalize\n",
    "rec1 = rec1/max(rec1)   # normalize\n",
    "rec2 = rec2/max(rec2)   # normalize\n",
    "\n",
    "## segment \n",
    "slice_x = rec1[:20000]\n",
    "slice_y = rec2[:20000]\n",
    "\n",
    "## estimate the impulse response\n",
    "x = channel(refSignal, slice_x)\n",
    "x = x / max(x)\n",
    "y = channel(refSignal, slice_y)\n",
    "y = y / max(y)\n",
    "\n",
    "## find the peak location\n",
    "x_index = np.argmax(x)\n",
    "y_index = np.argmax(y)\n",
    "\n",
    "## find the time difference between the two peaks\n",
    "tdoa_12 = (y_index - x_index) / Fs\n",
    "\n",
    "## find the distance between the two peaks\n",
    "dist_12 =  tdoa_12* 343\n",
    "\n",
    "## print results\n",
    "print(tdoa_12,dist_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**  Once you are satisfied with the performance of the basic algorithm, extend your code with ways\n",
    "to automatically segment the received data and find the beginning of a pulse. Hint: A useful\n",
    "function is findpeaks, which allows you to implement criteria for finding the first strong peak in\n",
    "a pulse sequence.\n",
    "\n",
    "**Task 4** Complete the TDOA function. It should return the time difference between any pairs of micro-\n",
    "phones. Use this function to find TDOAs between channel 0 and all other 4 recording pairs. Use these resulots to find the time difference between the other pairs as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 4\n",
    "def TDOA(rec1, rec2):\n",
    "\n",
    "        rec1 = rec1/max(rec1)   # normalize\n",
    "        rec2 = rec2/max(rec2)   # normalize\n",
    "\n",
    "        dists = np.zeros(C_repetition) # initialize array for distances\n",
    "        slice_size = len(rec1) // C_repetition # calculate slice size\n",
    "        for i in range(C_repetition):\n",
    "            slice_x = rec1[i*slice_size: i*slice_size+slice_size] \n",
    "            slice_y = rec2[i*slice_size: i*slice_size+slice_size]\n",
    "            x = channel(refSignal, slice_x)\n",
    "            x = x / max(x)\n",
    "            y = channel(refSignal, slice_y)\n",
    "            y = y / max(y)\n",
    "            x_index, y_index = tdoa(x, y)\n",
    "            dists[i] = (y_index - x_index) / Fs * 343\n",
    "\n",
    "        dists = average_of_3_median_values(dists)\n",
    "        return np.mean(dists)\n",
    "\n",
    "def tdoa(x, y):\n",
    "\n",
    "        x_index = np.argmax(x)\n",
    "        y_index = np.argmax(y)\n",
    "\n",
    "        return x_index, y_index\n",
    "\n",
    "def channel(x, y):\n",
    "        \"\"\"\n",
    "        Channel estimation using matched filtering.\n",
    "        \"\"\"\n",
    "        xr = x[::-1]\n",
    "        h = np.convolve(y, xr, mode='full')  # filter xr with y\n",
    "        alpha = np.dot(x.T, x)\n",
    "        hhat = h / alpha\n",
    "        return abs(hhat)\n",
    "\n",
    "def average_of_3_median_values(arr):\n",
    "        sorted_arr = np.sort(arr)\n",
    "        n = len(sorted_arr)\n",
    "\n",
    "        if n % 2 == 0:\n",
    "            # Even number of elements\n",
    "            mid1 = n // 2 - 1\n",
    "            mid2 = n // 2\n",
    "            mid3 = n // 2 + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "        else:\n",
    "            # Odd number of elements\n",
    "            mid = n // 2\n",
    "            mid1 = mid - 1\n",
    "            mid2 = mid\n",
    "            mid3 = mid + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "\n",
    "        average = np.mean(three_medians)\n",
    "        return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the other recordings\n",
    "Fs, refSignal = wavfile.read(\"Files/Student Recordings/reference.wav\")\n",
    "refSignal = refSignal[:,0]\n",
    "refSignal = refSignal / max(refSignal) # normalize\n",
    "refSignal = refSignal[0:4000]\n",
    "\n",
    "Fs, recording = wavfile.read(\"Files/Student Recordings/record_x150_y185.wav\")\n",
    "recording = recording [:200000,:]\n",
    "\n",
    "C_repetition = 9\n",
    "\n",
    "D12 = TDOA(recording[:, 0], recording[:, 1])\n",
    "D13 = TDOA(recording[:, 0], recording[:, 2])\n",
    "D14 = TDOA(recording[:, 0], recording[:, 3])\n",
    "D15 = TDOA(recording[:, 0], recording[:, 4])\n",
    "### @Mano order of 2 and 3 was reversed, shall we ask them to find distance and not time\n",
    "\n",
    "# Calculate other microphone differences\n",
    "D23 = (D13 - D12)\n",
    "D24 = (D14 - D12)\n",
    "D34 = (D14 - D13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable** In the midterm report, document your algorithms, show examples of measurements/channel\n",
    "estimates, and include a subsection on testing, showing your findings and accuracies.\n",
    "\n",
    "As plots, we suggest to show an entire recording, and then one where you zoom in on the short\n",
    "segment that you give to ch3. Of the resulting channel estimate, show the entire result, and then\n",
    "zoom in on the interesting part where are the peaks. Remember that for negative delays, the peak\n",
    "of interest will be at the “large n” part of your estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @B add a code block for each section, to complete the taseks, shall we create a class for it?\n",
    "## Shall we already add the localization module here and ask them to complete it?\n",
    "## maybe first as seperate function then integrated in the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOCALIZATION USING TDOA INFORMATION ##\n",
    "\n",
    "Now we arrive at the main question studied in this module: how can we locate the car using the TDOA\n",
    "estimates? With 5 microphones, we can compute the TDOAs between all pairs of microphones and\n",
    "obtain 10 TDOA pairs.\n",
    "Study Appendix C, (@B update) it shows a basic algorithm to compute the (x, y) location of the car based on the\n",
    "measured TDOAs. This algorithm can be extended to fit our situation: In our application, we have the\n",
    "audio beacon on the car at a certain height above the ground (we define this as z = 0), but the microphones\n",
    "are placed on stands at a different height. You can assume that the height difference between the audio\n",
    "beacon of the car and the microphones is known.\n",
    "\n",
    "## Localization algorithm assignments\n",
    "\n",
    "**Task 1** Develop a test code using Pythagoras’ theorem that takes an (x, y) position as input and calculates\n",
    "the TDOA that you would observe from this position. This involves calculating the distance from\n",
    "the given point to each microphone. This will help you debug both your TDOA function and your\n",
    "`coordinate_2d` function.\n",
    "\n",
    "**Task 2** \n",
    "Using the distsnces estimated previously and the microphone location generate matrix A and b (introduced in appendix C) and use them to find the (x,y) location. Once you are sure it works for a few examples turn it into a function called `coordinate_2d`.\n",
    "\n",
    "**Deliverable** Document the accuracy of your localization algorithm. Is this acceptable for your appli-\n",
    "cation? As an illustration, you can also plot the room and show the true locations and estimated\n",
    "locations. For the midterm report, document your algorithm and the results of the tests on simu-\n",
    "lated and given test data. Also report on the given test data with unknown position: what positions\n",
    "do you estimate? Finish with a conclusion that summarizes the accuracy that can be expected and\n",
    "the reliability (i.e., how often it succeeds in finding a reliable location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microphone coordinates\n",
    "xyMic = np.array([[0, 0], [4.80, 0], [0, 4.80], [4.80, 4.80]])\n",
    "\n",
    "A = np.array([[2 * (xyMic[1, 0] - xyMic[0, 0]), 2 * (xyMic[1, 1] - xyMic[0, 1]), -2 * D12, 0, 0],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[0, 0]), 2 * (xyMic[2, 1] - xyMic[0, 1]), 0, -2 * D13, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[0, 0]), 2 * (xyMic[3, 1] - xyMic[0, 1]), 0, 0, -2 * D14],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[1, 0]), 2 * (xyMic[2, 1] - xyMic[1, 1]), 0, -2 * D23, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[1, 0]), 2 * (xyMic[3, 1] - xyMic[1, 1]), 0, 0, -2 * D24],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[2, 0]), 2 * (xyMic[3, 1] - xyMic[2, 1]), 0, 0, -2 * D34]\n",
    "                      ])\n",
    "\n",
    "b = np.array([(pow(D12, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[1, :]), 2)),\n",
    "                      (pow(D13, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D14, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D23, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D24, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D34, 2) - pow(np.linalg.norm(xyMic[2, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2))\n",
    "                      ])\n",
    "\n",
    "y = np.linalg.inv(A.T @ A) @ A.T @ b\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization Class\n",
    "\n",
    "Now, it is time to start setting up a processing pipeline by putting everything together. The localization class you will develop will take a 5-channel recording of input and return the x and y coordinates of the car. Below is an outline of a localization class you could use. Note that this is to help you get started, and you will have to add methods and logic yourself. Look at the comments inside of the class for an explanation. Once you are done, you can copy this class in a seperate file and import and use it in other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization:\n",
    "    def __init__(self, recording, debug=False):\n",
    "    # Store the recordings\n",
    "    # Load the reference signal from memory\n",
    "    # x_car, y_car = self.localization()\n",
    "\n",
    "    def localization(self):\n",
    "    # Split each recording into individual pulses\n",
    "    # Calculate TDOA between different microphone pairs\n",
    "    # Run the coordinate_2d using the calculated TDOAs\n",
    "\n",
    "    def TDOA(self, rec1, rec2):\n",
    "    # Calculate channel estimation of each recording using ch2 or ch3\n",
    "    # Calculate TDOA between two recordings based on peaks\n",
    "    # in the channel estimate\n",
    "\n",
    "    @staticmethod\n",
    "    def channel(x, y):\n",
    "    # Channel estimation\n",
    "\n",
    "    def coordinate_2d(self, D12, D13, D14):\n",
    "    # Calculate 2D coordinates based on TDOA measurements\n",
    "    # using the linear algebra given before\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Main block for testing\n",
    "# Read the .wav file\n",
    "# Localize the sound source\n",
    "# Present the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization:\n",
    "\n",
    "    def __init__(self, recording, debug=False):\n",
    "\n",
    "        self.debug = debug\n",
    "        self.recording = recording\n",
    "\n",
    "        self.Fs, self.refSignal = wavfile.read(\"Recordings/refSignal.wav\") # reference signal\n",
    "        self.refSignal = self.refSignal / max(self.refSignal) # normalize\n",
    "        self.refSignal = self.refSignal[0:1500] # cut to one pulse (maybe 4000 is better)\n",
    "\n",
    "        self.bitcode = 'F3824D4D'  # transmitted bits [-]\n",
    "        self.F_carrier = 5000  # carrier frequency [Hz]\n",
    "        self.F_bit = 2000  # bit frequency [Hz]\n",
    "        self.C_repetition = 20  # repetition count [-]\n",
    "\n",
    "        self.xyCar = [481, 481] # car location [m]\n",
    "\n",
    "        self.localization()\n",
    "\n",
    "    def localization(self):\n",
    "        \"\"\"\n",
    "        Perform localization based on time difference of arrival (TDOA) measurements.\n",
    "\n",
    "        This method calculates the TDOA between the recorded audio signals from different microphones\n",
    "        and uses these measurements to estimate the coordinates of the source of the sound.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        D12 = self.TDOA(self.recording[:, 0], self.recording[:, 1])\n",
    "        D13 = self.TDOA(self.recording[:, 0], self.recording[:, 3])\n",
    "        D14 = self.TDOA(self.recording[:, 0], self.recording[:, 2])\n",
    "        D15 = self.TDOA(self.recording[:, 0], self.recording[:, 4])\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"D12: {D12}\")\n",
    "            print(f\"D13: {D13}\")\n",
    "            print(f\"D14: {D14}\")\n",
    "            print(f\"D15: {D15}\")\n",
    "\n",
    "        self.xyCar = self.coordinate_2d(D12, D13, D14)\n",
    "        print(f\"Location of car: {self.xyCar}\")\n",
    "\n",
    "    def TDOA(self, rec1, rec2):\n",
    "\n",
    "        rec1 = rec1/max(rec1)   # normalize\n",
    "        rec2 = rec2/max(rec2)   # normalize\n",
    "\n",
    "        if self.debug:\n",
    "            plt.plot(range(len(rec1)), rec1)\n",
    "            plt.plot(range(len(rec2)), rec2)\n",
    "            plt.title(\"Recordings\")\n",
    "            plt.show()\n",
    "\n",
    "        dists = np.zeros(self.C_repetition) # initialize array for distances\n",
    "        slice_size = len(rec1) // self.C_repetition # calculate slice size\n",
    "        for i in range(self.C_repetition):\n",
    "            slice_x = rec1[i*slice_size: i*slice_size+slice_size] \n",
    "            slice_y = rec2[i*slice_size: i*slice_size+slice_size]\n",
    "            x = self.ch2(self.refSignal, slice_x)\n",
    "            x = x / max(x)\n",
    "            y = self.ch2(self.refSignal, slice_y)\n",
    "            y = y / max(y)\n",
    "            x_index, y_index = self.tdoa(x, y)\n",
    "            dists[i] = (y_index - x_index) / self.Fs * 343\n",
    "\n",
    "            if i < 3 and self.debug:\n",
    "                plt.plot(range(len(x)), x, alpha=0.3)\n",
    "                plt.plot(range(len(y)), y, alpha=0.3)\n",
    "                plt.title(f\"Channel estimate {(y_index - x_index) / self.Fs * 343}\")\n",
    "                plt.scatter(x_index, x[x_index], c='blue', marker='o', alpha=0.5)\n",
    "                plt.scatter(y_index, y[y_index], c='red', marker='o', alpha=0.5)\n",
    "                plt.show()\n",
    "                print((y_index - x_index) / self.Fs * 343)\n",
    "\n",
    "        dists = self.average_of_3_median_values(dists)\n",
    "        return np.mean(dists)\n",
    "\n",
    "    def tdoa(self, x, y):\n",
    "\n",
    "        x_index = np.argmax(x)\n",
    "        y_index = np.argmax(y)\n",
    "\n",
    "        return x_index, y_index\n",
    "\n",
    "    @staticmethod\n",
    "    def ch2(x, y):\n",
    "        \"\"\"\n",
    "        Channel estimation using matched filtering.\n",
    "        \"\"\"\n",
    "        xr = x[::-1]\n",
    "        h = np.convolve(y, xr, mode='full')  # filter xr with y\n",
    "        alpha = np.dot(x.T, x)\n",
    "        hhat = h / alpha\n",
    "        return abs(hhat)\n",
    "\n",
    "    def average_of_3_median_values(self, arr):\n",
    "        sorted_arr = np.sort(arr)\n",
    "        n = len(sorted_arr)\n",
    "\n",
    "        if n % 2 == 0:\n",
    "            # Even number of elements\n",
    "            mid1 = n // 2 - 1\n",
    "            mid2 = n // 2\n",
    "            mid3 = n // 2 + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "        else:\n",
    "            # Odd number of elements\n",
    "            mid = n // 2\n",
    "            mid1 = mid - 1\n",
    "            mid2 = mid\n",
    "            mid3 = mid + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "\n",
    "        average = np.mean(three_medians)\n",
    "        return average\n",
    "\n",
    "    def coordinate_2d(self, D12, D13, D14):\n",
    "        # Calculate other microphone differences\n",
    "        D23 = (D13 - D12)\n",
    "        D24 = (D14 - D12)\n",
    "        D34 = (D14 - D13)\n",
    "\n",
    "        # Microphone coordinates\n",
    "        xyMic = np.array([[0, 0], [4.80, 0], [0, 4.80], [4.80, 4.80]])\n",
    "\n",
    "        A = np.array([[2 * (xyMic[1, 0] - xyMic[0, 0]), 2 * (xyMic[1, 1] - xyMic[0, 1]), -2 * D12, 0, 0],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[0, 0]), 2 * (xyMic[2, 1] - xyMic[0, 1]), 0, -2 * D13, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[0, 0]), 2 * (xyMic[3, 1] - xyMic[0, 1]), 0, 0, -2 * D14],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[1, 0]), 2 * (xyMic[2, 1] - xyMic[1, 1]), 0, -2 * D23, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[1, 0]), 2 * (xyMic[3, 1] - xyMic[1, 1]), 0, 0, -2 * D24],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[2, 0]), 2 * (xyMic[3, 1] - xyMic[2, 1]), 0, 0, -2 * D34]\n",
    "                      ])\n",
    "\n",
    "        b = np.array([(pow(D12, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[1, :]), 2)),\n",
    "                      (pow(D13, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D14, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D23, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D24, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D34, 2) - pow(np.linalg.norm(xyMic[2, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2))\n",
    "                      ])\n",
    "\n",
    "        y = np.linalg.inv(A.T @ A) @ A.T @ b\n",
    "        return y[0:2]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_t = time.perf_counter()\n",
    "\n",
    "    # Read the .wav file\n",
    "    Fs, recording = wavfile.read(\"Recordings/recording349152.wav\")\n",
    "\n",
    "    # Localize the sound source\n",
    "    loc = Localization(recording, True)\n",
    "\n",
    "    stop_t = time.perf_counter()\n",
    "    print(f\"Total time: {stop_t - start_t:0.4f}\")\n",
    "    print(f\"Location of car: {loc.xyCar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extension\n",
    "\n",
    "If you finish the basic assignment quickly and want to challenge yourself further, try adding additional\n",
    "functionality to the program. For example, you could look at one of the following aspects:\n",
    "\n",
    "- The current set of linear equations in Appendix C does not consider the height difference between\n",
    "the microphones and the car. This leads to a slight offset. Adding in the 5th microphone at a\n",
    "different height can fix this. Use logic and maths to adjust the matrices to accommodate this extra\n",
    "microphone and the z-axis.\n",
    "- The provided method in Appendix C is simple but unreliable for certain locations. What happens\n",
    "if the distance of the car to two microphones is equal (symmetry positions)? In that case, one\n",
    "column of the matrix that you try to invert is zero. During System Integration, you can search the\n",
    "literature and try to implement more advanced algorithms, e.g.,\n",
    "    - Stephen Bancroft, “An algebraic solution of the GSP equations”, IEEE Transactions on\n",
    "Aerospace and Electronic Systems, vol.21, no.7, pp.56-59, January 1985.\n",
    "    - Amir Beck, Petre Stoica, and Jian Li, “Exact and Approximate Solutions of Source Localiza-\n",
    "    tion Problems”, IEEE Transactions on Signal Processing, vol.56, no.5, pp. 1770-1778, May\n",
    "    2008.\n",
    "    The literature on this topic is actually very rich. The latter paper gives a good overview. You can\n",
    "also try to solve for the true solution using the nonlinear Least Squares optimization toolboxes.\n",
    "In previous years, some students have implemented a grid search, in which the room is partitioned\n",
    "into a dense grid of possible positions, and each location is tested against the TDOA data to find\n",
    "the best fit. You could do this in two steps: first coarse, then fine.\n",
    "\n",
    "- Try a different channel estimation function. \n",
    "- Apply filters to detect outliers and average the result of multiple pulses.\n",
    "\n",
    "If you have completed this successfully, you can start integrating and estimating the car’s location. Start\n",
    "by finding a set of suitable parameters for your audio beacon. Communicate with the other part of your\n",
    "team. They should be finalizing the microphone recording code. If this is not the case, work together to\n",
    "complete the code. It is better to complete the mid-term first with well-tested code and then to continue\n",
    "working on more functionality. Otherwise, you will spend all your time trying to integrate all the loose\n",
    "bits of code in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFTER THE MIDTERM: REFERENCE SIGNAL AND INTEGRATION ##\n",
    "(@B @mano @mehrdad how to be more clear about htis division, is it mandatory? it's so open-ended, should we add some more guidlines for the students?)\n",
    "\n",
    "As you will remember from your EE2T11 Telecommunications A practical in Q3 (?? @B update), you must have a clean\n",
    "reference signal to get a good channel/impulse response estimate, which you can then use to deconvolve\n",
    "the recordings you make to locate KITT. Getting this clean reference is what you will do here since it\n",
    "entails more than just simulating the OOK code, as you must also consider the microphone’s channel\n",
    "and the beacon’s behavior.\n",
    "This reference is crucial in finding the channel’s impulse response between the beacon and the micro-\n",
    "phone and, consequently, in finding the TDOA to locate KITT within the field.\n",
    "Appendix A reminds you of the beacon signal parameters that are used to generate the beacon signal;\n",
    "they are similar to what you saw in the EE2T11 practical.\n",
    "\n",
    "## Reference Signal Assessment\n",
    "\n",
    "**Task 1** Determine a good bit code to transmit using KITT’s beacon, for this consider its autocorrelation\n",
    "function. You want a strong peak for the 0-lag of the autocorrelation but as low as possible for any\n",
    "other lag.\n",
    "*Hint:* Suitable codes could be randomly generated, or you can try some optimal codes (check\n",
    "communication theory literature for “gold codes”).\n",
    "\n",
    "**Task 2** Determine the other parameters for KITT. What is a good carrier frequency to use? What is a good bit frequency? A perfect repetition count is not yet required as this will depend on your later script but do make sure that the full code can be transmitted and that it can be recorded on all microphones within the same recording window.\n",
    "*Hint:* You could try to find a datasheet of the microphones to determine a good carrier frequency;\n",
    "it depends on the sensitivity of the loudspeaker and the microphones. More reliably: you can measure the response of the system for various carrier frequencies (e.g., spaced by 1000 Hz) and make a plot of the amplitude response. You can then later select a carrier frequency with maximal\n",
    "response\n",
    "\n",
    "**Task 3** Record the bit-code transmitted over KITT’s beacon with your new parameters. Keep the micro- phone very close to KITT’s beacon to get a recording that is as clear as possible, but do make sure\n",
    "to avoid clipping. Look at the waveform you get from this recording. Can you think of a way to reduce the noise? If you cannot develop a way to do this, ask a TA or other instructors.\n",
    "\n",
    "**Task 4** The recording you made with the microphone close to the beacon can be the reference signal.\n",
    "Look at the waveform of the reference and look at its autocorrelation. What do you see? Compare this to the “ideal” OOK and autocorrelation from **Task 1**.\n",
    "\n",
    "**Task 5** Clean up your recording and strip “zero intervals” away such that only a clean recording of a single pulse remains.\n",
    "\n",
    "**Deliverable** Show the autocorrelations and plots you made in the final report and comment on what you see. Explain your answers to the questions, what led you to those answers, and explain your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration KITT assignments\n",
    "(@B @mano @mehrdad  should we add some more guidlines for the students? more methods to kiit or localization class)\n",
    "\n",
    "**Task 1** Record several transmissions from KITT’s beacon with KITT at various locations in the field.\n",
    "Store these recordings and KITT’s (x, y) coordinates.\n",
    "\n",
    "**Task 2** Using the reference signal you developed previously and the algorithms you developed for the mid-term, estimate the locations using these recordings, off-line.\n",
    "\n",
    "**Task 3** Do the same as in Task 2, but now couple the location algorithm and the recording code. Assuming you solve the “blocking recording” issue, you should now be able to drive KITT around and locate it all in realtime.\n",
    "\n",
    "**Task 4** Try to add time-stamp information. By the time you calculated your position, KITT has already moved. And note that in your recording, you look for a pulse, which is also from some time ago.\n",
    "Luckily, you can estimate most of these delays. Augmenting your location estimates with time stamps might be very helpful for the controller that your team will build next, in particular if you\n",
    "intend to drive fast.\n",
    "\n",
    "**Deliverable** Document this process and reflect on your findings and accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-term assessment 3 and report\n",
    "\n",
    "In week 4, you will have to showcase the functionality of your localization script to your assigned TA.\n",
    "You should demonstrate proper localization of the car on the 3 recordings with unknown coordinates.\n",
    "After you pass this assessment, you are ready to document your results in your midterm report. A\n",
    "detailed report is required, covering the approach, implementation, testing and results, as mentioned\n",
    "above. Please review Chapter 7 for guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preparation__\n",
    "- Read this module\n",
    "- Review your TDOA and deconvolution algorithms from the EE2T11 Telecommunications A(??@B update) practical\n",
    "- Study Appendix C (??@B update)\n",
    "- Make sure your Python IDE is ready to go!\n",
    "\n",
    "\n",
    "__Time duration__ Four lab sessions and four preparation/reporting sessions at home\n",
    "    \n",
    "__What is needed:__ The following\n",
    "- KITT\n",
    "- Laptop/PC running Python\n",
    "- Your TDOA and deconvolution algorithms from EE2T11 Telecommunications A\n",
    "- On Brightspace: 7+3 pre-recorded audio recordings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opened",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
