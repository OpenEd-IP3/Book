{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![book header](pictures/header.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt           # For plotting purposes\n",
    "import numpy as np                        # For convolution function\n",
    "from scipy.io import wavfile\n",
    "import time\n",
    "\n",
    "# from serial import Serial # Uncomment this line if you are using the real car\n",
    "# from pyaudio import PyAudio, paInt16 # Uncomment this line if you are using the real car\n",
    "from KITT_Simulator.py_audio_simulator import PyAudio, paInt16 # Uncomment this line if you are using the simulator\n",
    "from KITT_Simulator.serial_simulator import Serial # Uncomment this line if you are using the simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5:\n",
    "# Module 3 - Locating KITT Using Audio Communication\n",
    "\n",
    "__Learning objectives__ The following is learned and practiced in this module:\n",
    "- The importance of good auto-correlation properties of your bit-code\n",
    "- Channel estimation and TDOA from recorded signals\n",
    "- Location estimation algorithms based on TDOA\n",
    "\n",
    "    \n",
    "KITT must be located in its field and then directions must be determined to navigate to the final destination. In the previous modules your colleagues are developing scripts to communicate with KITT. They will add functionality to read the audio signals from the microphones located around the field, and you should use these to locate the car. It is recommended for all group members to read Modules 1 and 2 to have a better understanding of how everything should work together.\n",
    "\n",
    "For the localization, we will use (real-time) recordings of the beacon signal at the various microphones, deconvolve these using a reference signal recording, and determine the relative time delays from the resulting channel estimates. (It is assumed that you have working channel estimation algorithms from the two Assignments in Week 1.) Depending on the distance to each microphone, the signal transmitted by KITT’s beacon arrives a little bit earlier or later, and you can convert that into physical distances. For each pair of microphones, we can compute this time difference of arrival (TDOA), or the physical difference in propagation distance. If you have measurements from enough microphones, then you can calculate the location of KITT in the field.\n",
    "\n",
    "At the end of this Module, you will have developed a script to locate KITT within the field with reasonable accuracy, and you will do so using the data recorded by the microphones located along the field. You will also have tested and verified the accuracy and robustness of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-recorded data\n",
    "\n",
    "To get you started, 7 recodings at known locations and a reference recording taken close to one of the microphones will be provided in this task. These can be used to develop and test your algorithms. The recordings are at locations randomly distributed across the field, as follows: \n",
    "\n",
    "| |   x [cm]   |   y [cm]   |\n",
    "|--|-------|-------|\n",
    "|0|  64   |   40  |\n",
    "|1|  82   |  399  |\n",
    "|2| 109  |   76  |\n",
    "|3| 143  |  296  |\n",
    "|4|  150  |  185  |\n",
    "|5|  178  |  439  |\n",
    "|6|  232  |  275  |\n",
    "\n",
    "*Table 1: Locations of the given recordings (cm)*\n",
    "\n",
    "The x and y axis of the field are defined as follows, where the numbers refer to the microphone index:\n",
    "\n",
    "\n",
    " <img src=\"pictures/axisdef.png\" alt=\"mic-figure\" width=\"250px\">\n",
    "\n",
    "*Field axis and microphone index definition*\n",
    "<!--\n",
    "```{figure} axisdef.png\n",
    "---\n",
    "height/width: 150px\n",
    "name: mic-figure\n",
    "---\n",
    "Microphone Axis definition\n",
    "```\n",
    "-->\n",
    "\n",
    "You can assume these positions for the microphones. Note the different height of microphone 5.\n",
    "\n",
    "|Microphone|   x [cm]   |  y [cm] |  z [cm] |\n",
    "|-------|-------|-------|-------|\n",
    "|  1    |  0    | 0     |   50  |\n",
    "|  2    |  0    | 480   |   50  |\n",
    "|  3    |  480  | 480   |   50  |\n",
    "|  4    |  480  | 0     |   50  |\n",
    "|  5    |  0    | 240   |   80  |\n",
    "\n",
    "*Table 2: Location of the microphones (cm)*\n",
    "\n",
    "The code below helps you to load and plot the 7 audio signals provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_x = [64, 82, 109, 143, 150, 178, 232]\n",
    "record_y = [40, 399, 76, 296, 185, 439, 275]\n",
    "\n",
    "filename =[]\n",
    "\n",
    "for i in range(len(record_x)):\n",
    "    real_x = record_x[i] \n",
    "    real_y = record_y[i]\n",
    "    filename.append(\"Files/Student Recordings/record_x\" + str(real_x) + \"_y\" + str(real_y) + \".wav\")\n",
    "\n",
    "\n",
    "Fs, recording = wavfile.read(filename[0])\n",
    "plt.plot(recording[:,0])    # plot the first signal of the first recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background knowledge\n",
    "\n",
    "In the Assignments of Week 1, you developed algorithms for channel estimation for 2 microphone signals, both receiving a beacon (training) signal. In the present module, we extend this to 5 microphones and use this to locate the car.\n",
    "\n",
    "The channel estimation problem is the following: Suppose we transmit a known signal $x[n]$ over a communication channel, and measure the result $y[n]$.  The channel acts as a\n",
    "filter, which we will assume to be linear and time-invariant.\n",
    "Therefore, the measured signal is a convolution of the transmitted\n",
    "signal by the channel impulse response $h[n]$, such that\n",
    "$y[n] = h[n] \\ast x[n]$.  Knowing the transmitted signal $x[n]$, can\n",
    "we recover the impulse response of the communication channel from\n",
    "$y[n]$?  This is essentially an inversion problem.\n",
    "\n",
    "\n",
    "To estimate $h[n]$, three alternative algorithms were described:\n",
    "- __ch1__ Deconvolution in the time domain: Involves matrix inversion. It is computationally complex and requires lots of memory (easily more than what the available PCs can handle).\n",
    "\n",
    "- __ch2__ The matched filter: Avoids the matrix inversion. It is equal to computing the cross-correlation of\n",
    "$y[n]$ with $x[n]$. As cross-correlation is equivalent to a convolution with a reverse signal $x[−n]$, we have $\\hat{h}[n] = y[n] \\ast x[-n]$. The convolution can be computed efficiently using the FFT. Without noise, the resulting channel estimate is equal to the true\n",
    "channel convolved with the autocorrelation of the pulse, $r[n] = x[n] ∗ x[−n]$, namely $\\hat{h}[n] = h[n] \\ast r[n]$. Therefore, its performance\n",
    "heavily depends on having the correct (i.e., accurate) “reference” or “training” signal to correlate\n",
    "your measurements with, and the signal should have good autocorrelation properties: $x[n] ∗ x[−n]$\n",
    "should be close to a delta spike. Large sidelobes will lead to confusion.\n",
    "- __ch3__ Deconvolution in the frequency domain: Involves the FFT. It computes the same channel as via\n",
    "deconvolution in the time domain but is much more efficient: convolution in time becomes point-\n",
    "wise multiplication in frequency, hence for deconvolution in the frequency domain, we only need\n",
    "a pointwise division, followed by an IFFT to obtain the time-domain channel impulse response.\n",
    "\n",
    "**Note**\n",
    "FFT and IFFT lead to periodicities (the result is cyclic, and samples $x[n]$ at negative time reappear\n",
    "at the “large n” part of the signal). Since this method is similar to ch1 (but much more efficient),\n",
    "you can view this method as a matched filter, followed by a correction step that “inverts” the effect\n",
    "of the autocorrelation of the transmit pulse. However, since we should not divide by zero, you will\n",
    "also need to implement a “threshold” to set non-invertible or very small frequency coefficients of\n",
    "$x[n]$ to zero, which otherwise will lead to noise amplification. The performance depends on this\n",
    "threshold, which has to be chosen heuristically.\n",
    "\n",
    "It is assumed that you have working algorithms for estimating channel responses. We recommend using __ch3:__ deconvolution in the frequency domain.\n",
    "\n",
    "In each case, a reference signal $x[n]$ is required. We recommend using a recording close to the beacon because then $x[n]$ includes the loudspeaker and microphone responses as well, and in ch3 these get divided out. For the pre-made recordings, a high-quality reference is available.\n",
    "\n",
    "\n",
    "The deconvolution algorithm gives a channel estimate for the beacon path to each microphone. After this, we detect the first incoming path, which, assuming Line of Sight (LOS), corresponds to the propagation delay of the car beacon to each microphone. Unfortunately, we do not know the transmit time, so we only obtain relative propagation delays. If we take the difference of microphone delays, this unknown transmit time is eliminated, so we can obtain time difference of arrival (TDOA) samples for each microphone pair.\n",
    "In the next sections, we will use these to locate the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution ##\n",
    "\n",
    "The first step is to get your deconvolution algorithm operational and tested. Do these tasks with the given recordings from in the Files directory, or Brightspace.\n",
    "\n",
    "**Task 1**  Using the provided reference signal and the algorithms you developed during the Assignments in Week 1, deconvolve the recordings to get the channel impulse response for each\n",
    "microphone. You will need to segment the received data into individual pulses; for the moment,\n",
    "you can do that manually. For a few measured locations, plot examples of the segmented data and the deconvolved channels\n",
    "(e.g., 10 plots per recording: the inputs and outputs of your deconvolution algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first copy/paste your channel estimation from the earlier assignment here\n",
    "def channel(x, y): # this is ch2\n",
    "        \"\"\"\n",
    "        Channel estimation using matched filtering.\n",
    "        \"\"\"\n",
    "        xr = x[::-1]\n",
    "        h = np.convolve(y, xr, mode='full')  # filter xr with y\n",
    "        alpha = np.dot(x.T, x)\n",
    "        hhat = h / alpha\n",
    "        return hhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load, normalize and segment one period of the reference signal\n",
    "Fs, refSignal = wavfile.read(\"Files/Student Recordings/reference.wav\") # reference signal\n",
    "refSignal = refSignal[0:4000,0] # one pulse\n",
    "refSignal = refSignal / max(refSignal) # normalize\n",
    "plt.plot(refSignal)\n",
    "\n",
    "# load, normalize and segment one period of one of the microphone signals\n",
    "Fs, recordings = wavfile.read(\"Files/Student Recordings/record_x109_y76.wav\")\n",
    "slice_rec1 = recordings[:20000,0]\n",
    "slice_rec1 = slice_rec1 / max(slice_rec1) # normalize (although this is not quite needed)\n",
    "plt.plot(slice_rec1) #one pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find the impulse response using the channel estimation \n",
    "x = channel(refSignal, slice_rec1)\n",
    "x = x / max(abs(x))\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**  From the peaks in the channel estimates, determine the time of arrivals (TOAs) and store these\n",
    "in a table. Check the accuracies of these estimates. Hint: Obviously, you want to compare these\n",
    "estimates to your true TOAs. The problem is that the transmission time is unknown, so a direct\n",
    "comparison is impossible. You can either compare TDOAs (the pairwise differences of TOAs) or\n",
    "introduce a single unknown parameter (the time of transmission) and develop an error measure\n",
    "that is insensitive to it. This is harder, but you might find literature on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: TDOA\n",
    "### load, normalize, segment and estimate the channel for two different channels as before\n",
    "\n",
    "## Load two channels\n",
    "rec1=recording[:, 0]\n",
    "rec2=recording[:, 2]\n",
    "\n",
    "## normalize\n",
    "rec1 = rec1/max(rec1)   # normalize\n",
    "rec2 = rec2/max(rec2)   # normalize\n",
    "\n",
    "## segment \n",
    "slice_x = rec1[:20000]\n",
    "slice_y = rec2[:20000]\n",
    "\n",
    "## estimate the impulse response\n",
    "x = channel(refSignal, slice_x)\n",
    "x = x / max(x)\n",
    "y = channel(refSignal, slice_y)\n",
    "y = y / max(y)\n",
    "\n",
    "## find the peak locations\n",
    "x_index = np.argmax(abs(x))\n",
    "y_index = np.argmax(abs(y))\n",
    "\n",
    "## find the time difference between the two peaks\n",
    "tdoa_12 = (y_index - x_index) / Fs\n",
    "\n",
    "## find the distance between the two peaks (assuming speed of sound = 343 m/s)\n",
    "dist_12 =  tdoa_12* 343\n",
    "\n",
    "## print results\n",
    "print(tdoa_12,dist_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**  Once you are satisfied with the performance of the basic algorithm, extend your code with ways\n",
    "to automatically segment the received data and find the beginning of a pulse. \n",
    "\n",
    "**Hint:** A useful\n",
    "function is findpeaks, which allows you to implement criteria for finding the first strong peak in\n",
    "a pulse sequence.\n",
    "\n",
    "**Task 4** Complete the TDOA function. It should return the time difference between a pair of microphones. Use this function to find TDOAs between channel 0 and all other 4 recording pairs. Use these results to find the time difference between the other pairs as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 4\n",
    "def TDOA(rec1, rec2):\n",
    "\n",
    "        rec1 = rec1/max(rec1)   # normalize\n",
    "        rec2 = rec2/max(rec2)   # normalize\n",
    "\n",
    "        dists = np.zeros(C_repetition) # initialize array for distances\n",
    "        slice_size = len(rec1) // C_repetition # calculate slice size\n",
    "        for i in range(C_repetition):\n",
    "            slice_x = rec1[i*slice_size: i*slice_size+slice_size] \n",
    "            slice_y = rec2[i*slice_size: i*slice_size+slice_size]\n",
    "            x = channel(refSignal, slice_x)\n",
    "            x = x / max(abs(x))\n",
    "            y = channel(refSignal, slice_y)\n",
    "            y = y / max(abs(y))\n",
    "            x_index, y_index = tdoa(x, y)\n",
    "            dists[i] = (y_index - x_index) / Fs * 343\n",
    "\n",
    "        dists = average_of_3_median_values(dists)\n",
    "        return np.mean(dists)\n",
    "\n",
    "def tdoa(x, y):\n",
    "\n",
    "        x_index = np.argmax(abs(x))     # (can make this more robust using 'findpeaks')\n",
    "        y_index = np.argmax(abs(y))\n",
    "\n",
    "        return x_index, y_index\n",
    "\n",
    "def channel(x, y):\n",
    "        \"\"\"\n",
    "        Channel estimation using matched filtering.\n",
    "        \"\"\"\n",
    "        xr = x[::-1]\n",
    "        h = np.convolve(y, xr, mode='full')  # filter xr with y\n",
    "        alpha = np.dot(x.T, x)\n",
    "        hhat = h / alpha\n",
    "        return abs(hhat)\n",
    "\n",
    "def average_of_3_median_values(arr):\n",
    "        sorted_arr = np.sort(arr)\n",
    "        n = len(sorted_arr)\n",
    "\n",
    "        if n % 2 == 0:\n",
    "            # Even number of elements\n",
    "            mid1 = n // 2 - 1\n",
    "            mid2 = n // 2\n",
    "            mid3 = n // 2 + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "        else:\n",
    "            # Odd number of elements\n",
    "            mid = n // 2\n",
    "            mid1 = mid - 1\n",
    "            mid2 = mid\n",
    "            mid3 = mid + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "\n",
    "        average = np.mean(three_medians)\n",
    "        return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the other recordings\n",
    "Fs, refSignal = wavfile.read(\"Files/Student Recordings/reference.wav\")\n",
    "refSignal = refSignal[:,0]\n",
    "refSignal = refSignal / max(refSignal) # normalize\n",
    "refSignal = refSignal[0:4000]\n",
    "\n",
    "Fs, recording = wavfile.read(\"Files/Student Recordings/record_x150_y185.wav\")\n",
    "recording = recording [:200000,:]\n",
    "\n",
    "C_repetition = 9\n",
    "\n",
    "D12 = TDOA(recording[:, 0], recording[:, 1])\n",
    "D13 = TDOA(recording[:, 0], recording[:, 2])\n",
    "D14 = TDOA(recording[:, 0], recording[:, 3])\n",
    "D15 = TDOA(recording[:, 0], recording[:, 4])\n",
    "### @Mano order of 2 and 3 was reversed, shall we ask them to find distance and not time\n",
    "\n",
    "# Calculate other microphone differences\n",
    "D23 = (D13 - D12)\n",
    "D24 = (D14 - D12)\n",
    "D34 = (D14 - D13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable** In the midterm report, document your algorithms, show examples of measurements/channel\n",
    "estimates, and include a subsection on testing, showing your findings and accuracies. Regarding the accuracies: you can compare the estimated differences to the distances you would expect from the given true location to each microphone.\n",
    "\n",
    "As plots, we suggest to show an entire recording, and then one where you zoom in on the short\n",
    "segment that you give to ch3. Of the resulting channel estimate, show the entire result, and then\n",
    "zoom in on the interesting part where are the peaks. Remember that for negative delays, the peak\n",
    "of interest will be at the “large n” part of your estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @B add a code block for each section, to complete the tasks, shall we create a class for it?\n",
    "## Shall we already add the localization module here and ask them to complete it?\n",
    "## maybe first as seperate function then integrated in the class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization using TDOA information\n",
    "\n",
    "Now we arrive at the main question studied in this module: how can we locate the car using the TDOA\n",
    "estimates? With 5 microphones, we can compute the TDOAs between all pairs of microphones and\n",
    "obtain 10 TDOA pairs. Next, you need an algorithm to convert this into the $(x,y)$ location of the car. We offer two approaches for this:\n",
    " - Study Appendix C, which shows a basic algorithm to solve for $(x,y)$ using a linear algebra approach. This algorithm is sub-optimal but should be rather fast.\n",
    " - In the paragraph \"Optional extensions\" (below), a suggestion is given to do a grid search: scan all possible $(x,y)$ positions and see which position gives the best match to the estimated TDOAs. This is accurate but could be slow (depending on how you implement this). \n",
    "\n",
    "Although we compute the $(x,y)$ position, in reality the world is 3D and the microphones have a certain height above the height of the audio beacon (we define the beacon to sit at $z=0$). The algorithm in Appendix C does not take this height into account. However, this extension is straightforward (see under \"Optional extensions\").\n",
    "\n",
    "### Localization algorithm assignments\n",
    "\n",
    "**Task 1** Develop a test code using Pythagoras’ theorem that takes an (x, y) position as input and calculates\n",
    "the TDOA that you would observe from this position. This involves calculating the distance from\n",
    "the given point to each microphone. This will help you debug both your TDOA function and your\n",
    "`coordinate_2d` function (see below).\n",
    "\n",
    "**Task 2** \n",
    "Using the distances estimated previously and the microphone location, generate matrix A and vector b (introduced in appendix C) and use them to find the (x,y) location. Once you are sure it works for a few examples turn it into a function called `coordinate_2d`. As an illustration, you can also plot the room and show the true locations and estimated\n",
    "locations.\n",
    "\n",
    "**Task 3** Document the accuracy of your localization algorithm. Is this acceptable for your application?  For the midterm report, document your algorithm and the results of the tests on simulated and given test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microphone coordinates\n",
    "xyMic = np.array([[0, 0], [4.80, 0], [0, 4.80], [4.80, 4.80]])\n",
    "\n",
    "A = np.array([[2 * (xyMic[1, 0] - xyMic[0, 0]), 2 * (xyMic[1, 1] - xyMic[0, 1]), -2 * D12, 0, 0],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[0, 0]), 2 * (xyMic[2, 1] - xyMic[0, 1]), 0, -2 * D13, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[0, 0]), 2 * (xyMic[3, 1] - xyMic[0, 1]), 0, 0, -2 * D14],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[1, 0]), 2 * (xyMic[2, 1] - xyMic[1, 1]), 0, -2 * D23, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[1, 0]), 2 * (xyMic[3, 1] - xyMic[1, 1]), 0, 0, -2 * D24],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[2, 0]), 2 * (xyMic[3, 1] - xyMic[2, 1]), 0, 0, -2 * D34]\n",
    "                      ])\n",
    "\n",
    "b = np.array([(pow(D12, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[1, :]), 2)),\n",
    "                      (pow(D13, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D14, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D23, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D24, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D34, 2) - pow(np.linalg.norm(xyMic[2, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2))\n",
    "                      ])\n",
    "\n",
    "y = np.linalg.inv(A.T @ A) @ A.T @ b\n",
    "print(y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization class\n",
    "\n",
    "Now, it is time to start setting up a processing pipeline by putting everything together. The localization class you will develop will take a 5-channel recording of input and return the x and y coordinates of the car. Below is an outline of a localization class you could use. Note that this is to help you get started, and you will have to add methods and logic yourself. Look at the comments inside of the class for an explanation. Once you are done, you can copy this class in a separate file and import and use it in other modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization:\n",
    "    def __init__(self, recording, debug=False):\n",
    "    # Store the recordings\n",
    "    # Load the reference signal from memory\n",
    "    # x_car, y_car = self.localization()\n",
    "\n",
    "    def localization(self):\n",
    "    # Split each recording into individual pulses\n",
    "    # Calculate TDOA between different microphone pairs\n",
    "    # Run the coordinate_2d using the calculated TDOAs\n",
    "\n",
    "    def TDOA(self, rec1, rec2):\n",
    "    # Calculate channel estimation of each recording using ch2 or ch3\n",
    "    # Calculate TDOA between two recordings based on peaks\n",
    "    # in the channel estimate\n",
    "\n",
    "    @staticmethod\n",
    "    def channel(x, y):\n",
    "    # Channel estimation\n",
    "\n",
    "    def coordinate_2d(self, D12, D13, D14):\n",
    "    # Calculate 2D coordinates based on TDOA measurements\n",
    "    # using the linear algebra given before\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Main block for testing\n",
    "# Read the .wav file\n",
    "# Localize the sound source\n",
    "# Present the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localization:\n",
    "\n",
    "    def __init__(self, recording, debug=False):\n",
    "\n",
    "        self.debug = debug\n",
    "        self.recording = recording\n",
    "\n",
    "        self.Fs, self.refSignal = wavfile.read(\"Recordings/refSignal.wav\") # reference signal\n",
    "        self.refSignal = self.refSignal / max(self.refSignal) # normalize\n",
    "        self.refSignal = self.refSignal[0:1500] # cut to one pulse (maybe 4000 is better)\n",
    "\n",
    "        self.bitcode = 'F3824D4D'  # transmitted bits [-]\n",
    "        self.F_carrier = 5000  # carrier frequency [Hz]\n",
    "        self.F_bit = 2000  # bit frequency [Hz]\n",
    "        self.C_repetition = 20  # repetition count [-]\n",
    "\n",
    "        self.xyCar = [481, 481] # car location [m]\n",
    "\n",
    "        self.localization()\n",
    "\n",
    "    def localization(self):\n",
    "        \"\"\"\n",
    "        Perform localization based on time difference of arrival (TDOA) measurements.\n",
    "\n",
    "        This method calculates the TDOA between the recorded audio signals from different microphones\n",
    "        and uses these measurements to estimate the coordinates of the source of the sound.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        D12 = self.TDOA(self.recording[:, 0], self.recording[:, 1])\n",
    "        D13 = self.TDOA(self.recording[:, 0], self.recording[:, 3])\n",
    "        D14 = self.TDOA(self.recording[:, 0], self.recording[:, 2])\n",
    "        D15 = self.TDOA(self.recording[:, 0], self.recording[:, 4])\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"D12: {D12}\")\n",
    "            print(f\"D13: {D13}\")\n",
    "            print(f\"D14: {D14}\")\n",
    "            print(f\"D15: {D15}\")\n",
    "\n",
    "        self.xyCar = self.coordinate_2d(D12, D13, D14)\n",
    "        print(f\"Location of car: {self.xyCar}\")\n",
    "\n",
    "    def TDOA(self, rec1, rec2):\n",
    "\n",
    "        rec1 = rec1/max(rec1)   # normalize\n",
    "        rec2 = rec2/max(rec2)   # normalize\n",
    "\n",
    "        if self.debug:\n",
    "            plt.plot(range(len(rec1)), rec1)\n",
    "            plt.plot(range(len(rec2)), rec2)\n",
    "            plt.title(\"Recordings\")\n",
    "            plt.show()\n",
    "\n",
    "        dists = np.zeros(self.C_repetition) # initialize array for distances\n",
    "        slice_size = len(rec1) // self.C_repetition # calculate slice size\n",
    "        for i in range(self.C_repetition):\n",
    "            slice_x = rec1[i*slice_size: i*slice_size+slice_size] \n",
    "            slice_y = rec2[i*slice_size: i*slice_size+slice_size]\n",
    "            x = self.ch2(self.refSignal, slice_x)\n",
    "            x = x / max(x)\n",
    "            y = self.ch2(self.refSignal, slice_y)\n",
    "            y = y / max(y)\n",
    "            x_index, y_index = self.tdoa(x, y)\n",
    "            dists[i] = (y_index - x_index) / self.Fs * 343\n",
    "\n",
    "            if i < 3 and self.debug:\n",
    "                plt.plot(range(len(x)), x, alpha=0.3)\n",
    "                plt.plot(range(len(y)), y, alpha=0.3)\n",
    "                plt.title(f\"Channel estimate {(y_index - x_index) / self.Fs * 343}\")\n",
    "                plt.scatter(x_index, x[x_index], c='blue', marker='o', alpha=0.5)\n",
    "                plt.scatter(y_index, y[y_index], c='red', marker='o', alpha=0.5)\n",
    "                plt.show()\n",
    "                print((y_index - x_index) / self.Fs * 343)\n",
    "\n",
    "        dists = self.average_of_3_median_values(dists)\n",
    "        return np.mean(dists)\n",
    "\n",
    "    def tdoa(self, x, y):\n",
    "\n",
    "        x_index = np.argmax(x)\n",
    "        y_index = np.argmax(y)\n",
    "\n",
    "        return x_index, y_index\n",
    "\n",
    "    @staticmethod\n",
    "    def ch2(x, y):\n",
    "        \"\"\"\n",
    "        Channel estimation using matched filtering.\n",
    "        \"\"\"\n",
    "        xr = x[::-1]\n",
    "        h = np.convolve(y, xr, mode='full')  # filter xr with y\n",
    "        alpha = np.dot(x.T, x)\n",
    "        hhat = h / alpha\n",
    "        return abs(hhat)\n",
    "\n",
    "    def average_of_3_median_values(self, arr):\n",
    "        sorted_arr = np.sort(arr)\n",
    "        n = len(sorted_arr)\n",
    "\n",
    "        if n % 2 == 0:\n",
    "            # Even number of elements\n",
    "            mid1 = n // 2 - 1\n",
    "            mid2 = n // 2\n",
    "            mid3 = n // 2 + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "        else:\n",
    "            # Odd number of elements\n",
    "            mid = n // 2\n",
    "            mid1 = mid - 1\n",
    "            mid2 = mid\n",
    "            mid3 = mid + 1\n",
    "            three_medians = [sorted_arr[mid1], sorted_arr[mid2], sorted_arr[mid3]]\n",
    "\n",
    "        average = np.mean(three_medians)\n",
    "        return average\n",
    "\n",
    "    def coordinate_2d(self, D12, D13, D14):\n",
    "        # Calculate other microphone differences\n",
    "        D23 = (D13 - D12)\n",
    "        D24 = (D14 - D12)\n",
    "        D34 = (D14 - D13)\n",
    "\n",
    "        # Microphone coordinates\n",
    "        xyMic = np.array([[0, 0], [4.80, 0], [0, 4.80], [4.80, 4.80]])\n",
    "\n",
    "        A = np.array([[2 * (xyMic[1, 0] - xyMic[0, 0]), 2 * (xyMic[1, 1] - xyMic[0, 1]), -2 * D12, 0, 0],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[0, 0]), 2 * (xyMic[2, 1] - xyMic[0, 1]), 0, -2 * D13, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[0, 0]), 2 * (xyMic[3, 1] - xyMic[0, 1]), 0, 0, -2 * D14],\n",
    "                      [2 * (xyMic[2, 0] - xyMic[1, 0]), 2 * (xyMic[2, 1] - xyMic[1, 1]), 0, -2 * D23, 0],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[1, 0]), 2 * (xyMic[3, 1] - xyMic[1, 1]), 0, 0, -2 * D24],\n",
    "                      [2 * (xyMic[3, 0] - xyMic[2, 0]), 2 * (xyMic[3, 1] - xyMic[2, 1]), 0, 0, -2 * D34]\n",
    "                      ])\n",
    "\n",
    "        b = np.array([(pow(D12, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[1, :]), 2)),\n",
    "                      (pow(D13, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D14, 2) - pow(np.linalg.norm(xyMic[0, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D23, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[2, :]), 2)),\n",
    "                      (pow(D24, 2) - pow(np.linalg.norm(xyMic[1, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2)),\n",
    "                      (pow(D34, 2) - pow(np.linalg.norm(xyMic[2, :]), 2) + pow(np.linalg.norm(xyMic[3, :]), 2))\n",
    "                      ])\n",
    "\n",
    "        y = np.linalg.inv(A.T @ A) @ A.T @ b\n",
    "        return y[0:2]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_t = time.perf_counter()\n",
    "\n",
    "    # Read the .wav file\n",
    "    Fs, recording = wavfile.read(\"Recordings/recording349152.wav\")\n",
    "\n",
    "    # Localize the sound source\n",
    "    loc = Localization(recording, True)\n",
    "\n",
    "    stop_t = time.perf_counter()\n",
    "    print(f\"Total time: {stop_t - start_t:0.4f}\")\n",
    "    print(f\"Location of car: {loc.xyCar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extensions\n",
    "\n",
    "If you finish the basic assignment quickly and want to challenge yourself further, try adding additional\n",
    "functionality to the program. For example, you could look at one of the following aspects:\n",
    "\n",
    "- The current set of linear equations in Appendix C does not consider the height difference between\n",
    "the microphones and the car. This leads to a slight offset, in particular if the car is close to a microphone. You can augment the equations using a $z$ variable (height of the car) in the solution vector, and then use that you know that $z=0$ to eliminate this variable. Use this kind of logic and maths to adjust the matrices.\n",
    "- The provided method in Appendix C is simple but unreliable for certain locations. What happens\n",
    "if the distance of the car to two microphones is equal (symmetry positions)? In that case, one\n",
    "column of the matrix that you try to invert is zero. During System Integration, you can search the\n",
    "literature and try to implement more advanced algorithms, e.g.,\n",
    "    - Stephen Bancroft, “An algebraic solution of the GSP equations”, IEEE Transactions on\n",
    "Aerospace and Electronic Systems, vol.21, no.7, pp.56-59, January 1985.\n",
    "    - Amir Beck, Petre Stoica, and Jian Li, “Exact and Approximate Solutions of Source Localization Problems”, IEEE Transactions on Signal Processing, vol.56, no.5, pp. 1770-1778, May 2008.\n",
    "\n",
    "The literature on this topic is actually very rich. The latter paper gives a good overview. \n",
    "\n",
    "- Instead of the method in Appendix C, you could implement a grid search, in which the room is partitioned into a dense grid of possible positions, and each location is tested against the TDOA data to find the best fit. You could do this in two steps: first coarse (steps of 10 cm), then fine. This brute-force method is robust and quite accurate.\n",
    "\n",
    "- Try a different channel estimation function. \n",
    "- Apply filters to detect outliers and average the result of multiple pulses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid-term assessment 3 and report\n",
    "\n",
    "In week 4, you will have to showcase the functionality of your localization script to your assigned TA.\n",
    "You should demonstrate proper localization of the car on the 3 recordings with unknown coordinates.\n",
    "After you pass this assessment, you are ready to document your results in your midterm report. A\n",
    "detailed report is required, covering the approach, implementation, testing and results, as mentioned\n",
    "above. Finish with a conclusion that summarizes the accuracy that can be expected and\n",
    "the reliability (i.e., how often it succeeds in finding a reliable location).\n",
    "Please review Chapter 7 for guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After the Midterm: Reference signal and integration\n",
    "\n",
    "If you have completed this Module successfully, you can start integrating and estimating the car’s location from your own microphone measurements. Start by finding a set of suitable parameters for your audio beacon. Communicate with the other part of your team. They should be finalizing the microphone recording code. If this is not the case, work together to complete the code. It is better to complete the mid-term first with well-tested code and then to continue working on more functionality. Otherwise, you will spend all your time trying to integrate all the loose bits of code in the end.\n",
    "\n",
    "As you will remember from your EE2T11 Telecommunications A practical in Q3 (?? @B update), you must have a clean reference signal to get a good channel/impulse response estimate, which you can then use to deconvolve the recordings you make to locate KITT. Getting this clean reference is what you will do here since it entails more than just simulating the OOK code, as you must also consider the microphone’s channel and the beacon’s behavior.\n",
    "This reference is crucial in finding the channel’s impulse response between the beacon and the micro-\n",
    "phone and, consequently, in finding the TDOA to locate KITT within the field.\n",
    "Appendix A reminds you of the beacon signal parameters that are used to generate the beacon signal;\n",
    "they are similar to what you saw in the EE2T11 practical.\n",
    "\n",
    "### Reference signal selection\n",
    "\n",
    "Using a good beacon signal is important, because it determines the quality of your location estimates. Since the time in IP3 is limited, we will give a few general hints, but note that the topic could be explored in much more detail.\n",
    "\n",
    "In your ch3 algorithm (deconvolution in frequency domain), you have seen that we omit all frequencies where the beacon signal amplitude spectrum $|X(F)|$ is small, because we divide by $X(F)$ and this would blow up the noise. For best results, we want $X(F)$ to be large for all frequencies. Therefore, ideally, we have a flat power spectrum. For random signals, this corresponds to the use of \"white noise\".   \n",
    "\n",
    "**Beacon code** Our beacon code consists of 32 bits, each 0 or 1. White noise corresponds to a beacon code for which the autocorrelation resembles a delta spike. You could generate a bit code using 'rand', and then round to 0 or 1. A further consideration is that the code should start and end with '1', or else you are using in fact a code that is shorter than 32 bits. \n",
    "\n",
    "After generating a code, check its autocorrelation function, $x[n]\\ast x[-n]$. \n",
    "You want a strong peak for the 0-lag of the autocorrelation but as low as possible for any\n",
    "other lag.\n",
    "*Hint:* Randomly generated codes are suitable for our purposes. You could try some optimal codes (check\n",
    "communication theory literature for “gold codes”).\n",
    "\n",
    "**Carrier frequency and bit frequency** The carrier frequency defines the \"pitch\" of the transmitted signal. Since we use audio equipment which is optimized for human listening, it is better to use carriers below 10 kHz. \n",
    "\n",
    "The bit frequency determines the bandwidth of the transmitted signal. Since we want to probe the channel on as many different frequencies as possible, we would select a large but frequency (but it can't be larger than the carrier frequency). The hardware implementation seems to limit the total output power. If we use a wider bandwidth, the energy per hertz is reduced. Thus, there is a trade-off. You can try to find good values experimentally, i.e. try a range of bit frequencies in steps of 1000 Hz, and see which setting gives most accurate results.\n",
    "\n",
    "A perfect repetition count is not yet required; but you need to make sure that the full code can be transmitted and recorded on all microphones within the same recording window.\n",
    "\n",
    "### Integration assignment\n",
    "\n",
    "**Make test recording** Record the signal transmitted over KITT’s beacon with your newly selected parameters. Keep the microphone very close to KITT’s beacon to get a recording that is as clear as possible, but do make sure to avoid clipping.\n",
    "\n",
    "After loading the recording into python, clean it up: strip “zero intervals” away such that only a clean recording of a single pulse remains. This will be your reference signal to be used in deconvolution.\n",
    "\n",
    "**Test performance** With your car at a know location (e.g. the center of the field), make recordings and run your localization algorithm. Check the accuracy. If it seems insufficient, try new beacon parameters.\n",
    "\n",
    "**Integrate** Couple the location algorithm and the recording code. You will need to solve the “blocking recording” issue. You should now be able to drive KITT around and locate it all in realtime.\n",
    "\n",
    "**Optional refinement** Try to add time-stamp information. By the time you calculated your position, KITT has already moved. And note that in your recording, you look for a pulse, which is also from some time ago.\n",
    "Luckily, you can estimate most of these delays. Augmenting your location estimates with time stamps might be very helpful for the controller that your team will build next, in particular if you\n",
    "intend to drive fast.\n",
    "\n",
    "**Deliverable (final report)** Show your selected beacon parameters and comment on the resulting performance. How accurate is your localization algorithm? Are you able to simultaneously drive and locate in realtime?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opened",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
